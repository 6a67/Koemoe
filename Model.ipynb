{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.all import *\n",
    "from fastai.vision.all import *\n",
    "from fastxtend.audio.all import *\n",
    "from fastxtend.vision.all import *\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "T = TypeVar('T')\n",
    "Listified = Union[T, Iterable[T], MutableSequence[T], L, fastuple]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingWithWarmup(TrackerCallback):\n",
    "    \"A `TrackerCallback` that terminates training when monitored quantity stops improving.\"\n",
    "    order=TrackerCallback.order+3\n",
    "    def __init__(self, \n",
    "        monitor='valid_loss', # value (usually loss or metric) being monitored.\n",
    "        comp=None, # numpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n",
    "        min_delta=0., # minimum delta between the last monitor value and the best monitor value.\n",
    "        patience=1, # number of epochs to wait when training has not improved model.\n",
    "        warmup=5,\n",
    "        reset_on_fit=True, # before model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n",
    "    ):\n",
    "        super().__init__(monitor=monitor, comp=comp, min_delta=min_delta, reset_on_fit=reset_on_fit)\n",
    "        self.patience = patience\n",
    "        self.warmup = warmup\n",
    "\n",
    "    def before_fit(self): \n",
    "        self.wait = 0\n",
    "        self.warmup_wait = self.warmup\n",
    "        super().before_fit()\n",
    "    def after_epoch(self):\n",
    "        \"Compare the value monitored to its best score and maybe stop training.\"\n",
    "        super().after_epoch()\n",
    "        self.warmup_wait = max(0, self.warmup_wait - 1)\n",
    "        if self.new_best: self.wait = 0\n",
    "        else:\n",
    "            if self.warmup_wait == 0:\n",
    "                self.wait += 1\n",
    "            if self.wait >= self.patience and self.warmup_wait == 0:\n",
    "                print(f'No improvement since epoch {self.epoch - self.wait}: early stopping')\n",
    "                raise CancelFitException()\n",
    "\n",
    "def save_model(learn: Learner, name, pickle=False):\n",
    "    timestr = time.strftime(\"%m-%d-%Y\")\n",
    "    file_name = timestr + \"/\" + name\n",
    "    os.makedirs(\"models/\" + timestr, exist_ok=True)\n",
    "    saved = learn.save(file_name)\n",
    "    learn.save(\"latest\")\n",
    "    torch.save(learn.model, \"checkpoints/latest.pt\", )\n",
    "    if pickle:\n",
    "        os.makedirs(\"checkpoints/\" + timestr, exist_ok=True)\n",
    "        learn.export(\"checkpoints/\" + file_name + \".pkl\")\n",
    "        learn.export(\"checkpoints/latest.pkl\")\n",
    "    return saved, file_name\n",
    "\n",
    "class SaveBestState(TrackerCallback):\n",
    "    \"A `TrackerCallback` that saves the model's best during training and loads it at the end.\"\n",
    "    order = TrackerCallback.order+1\n",
    "    def __init__(self, \n",
    "        monitor='valid_loss', # value (usually loss or metric) being monitored.\n",
    "        comp=None, # numpy comparison operator; np.less if monitor is loss, np.greater if monitor is metric.\n",
    "        min_delta=0., # minimum delta between the last monitor value and the best monitor value.\n",
    "        fname='model', # model name to be used when saving model.\n",
    "        every_epoch=False, # if true, save model after every epoch; else save only when model is better than existing best.\n",
    "        at_end=False, # if true, save model when training ends; else load best model if there is only one saved model.\n",
    "        with_opt=False, # if true, save optimizer state (if any available) when saving model. \n",
    "        reset_on_fit=True # before model fitting, reset value being monitored to -infinity (if monitor is metric) or +infinity (if monitor is loss).\n",
    "    ):\n",
    "        super().__init__(monitor=monitor, comp=comp, min_delta=min_delta, reset_on_fit=reset_on_fit)\n",
    "        assert not (every_epoch and at_end), \"every_epoch and at_end cannot both be set to True\"\n",
    "        # keep track of file path for loggers\n",
    "        self.last_saved_path = None\n",
    "        self.last_saved_names = None\n",
    "        store_attr('fname,every_epoch,at_end,with_opt')\n",
    "\n",
    "    def _save(self, name): self.last_saved_path, self.last_saved_name = save_model(self.learn, name)\n",
    "\n",
    "    def _load_last(self):  self.learn.load(f'{self.fname}', with_opt=self.with_opt)\n",
    "    \n",
    "    def after_epoch(self):\n",
    "        \"Compare the value monitored to its best score and save if best.\"\n",
    "        if self.every_epoch:\n",
    "            if (self.epoch%self.every_epoch) == 0: self._save(self.fname)\n",
    "        else: #every improvement\n",
    "            super().after_epoch()\n",
    "            if self.new_best:\n",
    "                print(f'Better model found at epoch {self.epoch} with {self.monitor} value: {self.best}.')\n",
    "                self._save(f'{self.fname}')\n",
    "                print(f'Saved Path: {self.last_saved_path} Saved Name: {self.last_saved_name}')\n",
    "\n",
    "    def after_fit(self, **kwargs):\n",
    "        \"Load the best model.\"\n",
    "        if self.at_end: self._save(f'{self.fname}')\n",
    "        elif not self.every_epoch: self.learn.load(f'{self.last_saved_name}', with_opt=self.with_opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_acc(y_pred, y):\n",
    "    y,y_pred = TensorBase(y), TensorBase(y_pred)\n",
    "    \n",
    "    threshold = .5\n",
    "    \n",
    "    classes = int(y.shape[2] / 3)\n",
    "    \n",
    "    binary_true = torch.stack([y[:, :, x*3] for x in range(classes)], axis=1)\n",
    "    binary_pred = torch.stack([y_pred[:, :, x*3] for x in range(classes)], axis=1)\n",
    "    \n",
    "    binary_true = torch.greater_equal(binary_true, threshold)\n",
    "    binary_pred = torch.greater_equal(binary_pred, threshold)\n",
    "    \n",
    "    acc = (binary_true == binary_pred).type(torch.float32)\n",
    "    acc = torch.mean(acc, (-1, -2, -3))\n",
    "    \n",
    "    return acc\n",
    "\n",
    "def YOHOLoss(y_pred, y, beta=1.0):\n",
    "    #beta changes how much of the regression loss counts\n",
    "    # (bs, 30, 9) tensor shape\n",
    "    y,y_pred = TensorBase(y), TensorBase(y_pred)\n",
    "    \n",
    "    classes = int(y.shape[2] / 3)\n",
    "    \n",
    "    squared_diff = torch.square(y_pred - y) \n",
    "    \n",
    "    stack = []\n",
    "    for c in range(classes):\n",
    "        ss0 = squared_diff[:, :, c*3] * 0 + 1 #all 1s for binary label squared loss filter (they all count)\n",
    "        ss1 = y[:, :, c*3] * beta #acts as a filter for regression loss\n",
    "        ss2 = y[:, :, c*3] * beta\n",
    "        stack += [ss0, ss1, ss2]\n",
    "    \n",
    "    sss = torch.stack(stack, dim=2)\n",
    "    squared_diff = torch.multiply(squared_diff, sss)\n",
    "    \n",
    "    err = torch.sum(squared_diff, (-1, -2)) #sum the error for each time step, all the time steps\n",
    "    #err = torch.mean(err, -1) #mean along the time steps\n",
    "    #err = torch.sqrt(err)\n",
    "    err = torch.mean(err) #mean among batches, we want loss to be independent of batch size\n",
    "    if not err:\n",
    "        print(\"ERROR NAN\")\n",
    "        print(y)\n",
    "        print(y_pred)\n",
    "    return err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DatasetOptions:\n",
    "    categories: int = 2\n",
    "    clip_duration: int = 6\n",
    "    time_steps: int = 30\n",
    "    \n",
    "@dataclass\n",
    "class TrainStage:\n",
    "    name: str\n",
    "    epochs: int = 10\n",
    "    warmup: int = 5\n",
    "    patience: int = -1\n",
    "    beta: float = 1.0\n",
    "    freeze: bool = True\n",
    "    find_lr: bool = True\n",
    "    lr_suggestion: str = \"steep\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.patience < 0:\n",
    "            self.patience = self.epochs\n",
    "        \n",
    "@dataclass\n",
    "class ModelConfig:\n",
    "    name: str\n",
    "    batch_size: int\n",
    "    output_channels: int\n",
    "    pretrain: bool = True\n",
    "    stages: list[TrainStage] = field(default_factory=list)\n",
    "    train_stage: int = field(default=0)\n",
    "    frozen: bool = field(default=True)\n",
    "    lr: float = field(default=1e-5)\n",
    "    save_best: SaveBestState = None\n",
    "    \n",
    "    def can_train(self):\n",
    "        return self.train_stage < len(self.stages) - 1\n",
    "       \n",
    "    def train(self, learner: Learner):\n",
    "        if self.can_train():\n",
    "            stage: TrainStage = self.stages[self.train_stage]\n",
    "            if stage.freeze and not self.frozen:\n",
    "                learner.freeze()\n",
    "                self.frozen = True\n",
    "            elif not stage.freeze and self.frozen:\n",
    "                learner.unfreeze()\n",
    "                self.frozen = False\n",
    "                \n",
    "            print(f'\\n ======= {self.name} - {stage.name} |-| b={stage.beta},warmup={stage.warmup},patience={stage.patience},frozen={self.frozen} ======= ')\n",
    "            \n",
    "            loss_func = partial(YOHOLoss, beta=stage.beta)\n",
    "            learner.loss_func = loss_func\n",
    "            \n",
    "            if stage.find_lr:\n",
    "                print(\" --- LR Find ---\")\n",
    "                lrs: namedtuple = learner.lr_find(suggest_funcs=(minimum, steep, valley, slide))\n",
    "                lrs_dict: dict = lrs._asdict()\n",
    "                print(f' --- Found LR(s) --> {lrs} --- ')\n",
    "                print(f'{stage.lr_suggestion} Rate --> {lrs_dict[stage.lr_suggestion]}')\n",
    "                self.lr = lrs_dict[stage.lr_suggestion]\n",
    "                \n",
    "            \n",
    "            \n",
    "            print(\" --- Fit One Cycle ---\")\n",
    "            #Store save best callback so that the \"best\" is actually the best across the stages\n",
    "            if self.save_best:\n",
    "                save_best = self.save_best\n",
    "            else:\n",
    "                save_best = SaveBestState(fname=self.name + \".\" + stage.name, reset_on_fit=False)\n",
    "                self.save_best = save_best \n",
    "                \n",
    "            early_stop = EarlyStoppingWithWarmup(monitor=\"valid_loss\", patience=stage.patience, warmup=stage.warmup, reset_on_fit=False)\n",
    "            callbacks = [ShowGraphCallback(), save_best, early_stop]\n",
    "            learner.fit_one_cycle(stage.epochs, self.lr, cbs=callbacks)\n",
    "            \n",
    "            self.train_stage += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_fake_output(time_steps, categories):\n",
    "    t = np.zeros((time_steps, categories*3))\n",
    "    for c in range(categories):\n",
    "        for r in range(t.shape[0]):\n",
    "            t[r, c*3] = np.random.choice([0, 1])\n",
    "            t[r, c*3 + 1] = np.random.uniform(low=0, high=1.0)\n",
    "            t[r, c*3 + 2] = np.random.uniform(low=t[r, c*3 + 1], high=1.0)\n",
    "        \n",
    "    return [t]\n",
    "\n",
    "def alter_output(output, alter_binary=.15, alter_times=0):\n",
    "    altered = np.copy(output)[0]\n",
    "    time_steps = output[0].shape[0]\n",
    "    categories = int(output[0].shape[1] / 3)\n",
    "    \n",
    "    for c in range(categories):\n",
    "        for r in range(time_steps):\n",
    "            if np.random.rand() <= alter_binary:\n",
    "                altered[r, c*3] = (1 - altered[r, c*3])\n",
    "            if np.random.rand() <= alter_times:\n",
    "                altered[r, c*3 + 1] = np.random.uniform(low=0, high=1.0)\n",
    "                altered[r, c*3 + 2] = np.random.uniform(low=altered[r, c*3 + 1], high=1.0)\n",
    "        \n",
    "    return [altered]\n",
    "    \n",
    "def get_dls(ds, bs, ds_options: DatasetOptions):\n",
    "    dls = DataBlock(\n",
    "        blocks=(MelSpecBlock(sr=16000, hop_length=160, n_fft=2048), RegressionBlock(n_out=3*ds_options.categories*ds_options.time_steps)),\n",
    "        get_x=ColReader('File'),\n",
    "        get_y=ColReader(0),\n",
    "        splitter=RandomSplitter(valid_pct=0.2, seed=42),\n",
    "        item_tfms=RandomCropPad(ds_options.clip_duration, samples=16000*ds_options.clip_duration, padmode=AudioPadMode.Constant),\n",
    "        batch_tfms=[AmplitudeToDB(top_db=80), FrequencyMasking(max_mask=.2), Volume(p=.75, gain_range=(-18, 6))]\n",
    "    )\n",
    "    return dls.dataloaders(ds, bs=bs, num_workers=num_cpus(), pin_memory=True, verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_configs = {\n",
    "    0: ModelConfig(\n",
    "        \"convnext_small.fb_in22k_ft_in1k\", \n",
    "        batch_size=64,\n",
    "        output_channels=768,\n",
    "        pretrain=True,\n",
    "        stages = [\n",
    "            TrainStage(\"stage0-0\", epochs=16, beta=1, freeze=True, lr_suggestion=\"valley\"), \n",
    "            TrainStage(\"stage0-1\", epochs=5, beta=1, freeze=True, lr_suggestion=\"minimum\"), \n",
    "            \n",
    "            TrainStage(\"stage1\", epochs=50, patience=5, warmup=5, freeze=False, lr_suggestion=\"steep\"),\n",
    "            TrainStage(\"stage2\", epochs=5, beta=2, freeze=True, find_lr=False), #regression focus\n",
    "            TrainStage(\"stage3\", epochs=5, beta=2e-1, freeze=True, find_lr=False), #binary focus\n",
    "            TrainStage(\"stage4\", epochs=5, beta=1, freeze=True, find_lr=False), #balanced focus\n",
    "        ]\n",
    "    ),\n",
    "    1: ModelConfig(\n",
    "        \"convnextv2_base.fcmae_ft_in22k_in1k_384\", \n",
    "        batch_size=64,\n",
    "        output_channels=1024,\n",
    "        pretrain=True,\n",
    "        stages = [\n",
    "            TrainStage(\"stage0-0\", epochs=30, patience=10, warmup=5, beta=1, freeze=True, lr_suggestion=\"valley\"), \n",
    "            TrainStage(\"stage0-1\", epochs=20, patience=7, warmup=5, beta=1, freeze=True, lr_suggestion=\"valley\"), \n",
    "            \n",
    "            TrainStage(\"stage1\", epochs=50, patience=5, warmup=5, freeze=False, lr_suggestion=\"minimum\"),\n",
    "            TrainStage(\"stage2\", epochs=20, patience=5, warmup=0, beta=2, freeze=True, find_lr=False),\n",
    "            TrainStage(\"stage2\", epochs=20, patience=5, warmup=0, beta=1e-2, freeze=True, find_lr=False),\n",
    "        ]\n",
    "    ),\n",
    "}\n",
    "\n",
    "mod_cfg = model_configs[0]\n",
    "ds_options = DatasetOptions(categories=2, clip_duration=6, time_steps=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "df = pd.read_csv(\"X:\\ML\\Datasets\\koe\\labels.csv\")\n",
    "df2 = df.groupby('File', sort=False).apply(np.array, include_groups=False).reset_index()#.apply(np.ravel).reset_index()#\n",
    "\n",
    "dls = get_dls(df2, bs=mod_cfg.batch_size, ds_options=ds_options)\n",
    "\n",
    "#output of the base_model will be 4 x 18 x 1024 = 4 x duration*3 x 1024\n",
    "last_channels = mod_cfg.output_channels\n",
    "\n",
    "head_def = []\n",
    "\n",
    "# head_def = [\n",
    "#     ([3, 3], 1, 1024),\n",
    "#     ([3, 3], 1, 512),\n",
    "#     ([3, 3], 1, 256),\n",
    "#     ([3, 3], 1, 128),\n",
    "#     #([3, 3], 1, 64),\n",
    "# ]\n",
    "\n",
    "c = last_channels\n",
    "while c >= 128:\n",
    "    head_def.append(([3, 3], 1, int(c)))\n",
    "    c = c/2\n",
    "\n",
    "mod_dict = OrderedDict()\n",
    "for i in range(len(head_def)):\n",
    "    l = head_def[i]\n",
    "    \n",
    "    mod_dict[str(i) + '-' + \"conv2d_dw\"] = nn.Conv2d(kernel_size=l[0], stride=l[1], in_channels=last_channels, out_channels=last_channels, groups=last_channels, padding=\"same\") #depthwise\n",
    "    mod_dict[str(i) + '-' + \"batchnorm_0\"] = batch_norm_0 = nn.BatchNorm2d(eps=1e-4, num_features=last_channels)\n",
    "    mod_dict[str(i) + '-' + \"relu_0\"] = nn.ReLU()\n",
    "    mod_dict[str(i) + '-' + \"conv2d\"] = nn.Conv2d(in_channels=batch_norm_0.num_features, out_channels=l[2],  kernel_size=[1, 1], stride=1, padding=\"same\")\n",
    "    mod_dict[str(i) + '-' + \"batchnorm_1\"] = nn.BatchNorm2d(eps=1e-4, num_features=l[2])\n",
    "    mod_dict[str(i) + '-' + \"relu_1\"] = nn.ReLU()\n",
    "    \n",
    "    last_channels = l[2]\n",
    "\n",
    "mod_dict[\"reshape\"] = nn.Flatten(start_dim=1, end_dim=2) #reshape 64 x 4 x 18 into 256 x 18 etc\n",
    "mod_dict[\"adaptive2dpool\"] = nn.AdaptiveAvgPool2d(output_size=(256, ds_options.categories*3))\n",
    "mod_dict[\"conv1d\"] = nn.Conv1d(256, out_channels=ds_options.time_steps, kernel_size=1)\n",
    "mod_dict[\"sigmoid\"] = nn.Sigmoid()\n",
    "custom_head = nn.Sequential(mod_dict)\n",
    "\n",
    "learn = vision_learner(dls, mod_cfg.name, opt_func=adam(foreach=True), custom_head=custom_head, pretrained=mod_cfg.pretrain, loss_func=YOHOLoss, metrics=[binary_acc], n_in=1, pool=False, concat_pool=False).to_channelslast()\n",
    "\n",
    "#learn.model\n",
    "#learn.summary()\n",
    "\n",
    "while mod_cfg.can_train():\n",
    "    mod_cfg.train(learn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
