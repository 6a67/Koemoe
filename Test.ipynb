{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import math\n",
    "import torch\n",
    "import ffprobe3\n",
    "import shutil\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from ffmpeg import FFmpeg, FFmpegError # type: ignore\n",
    "from IPython.display import Audio\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "input_file = Path(\"X:/ML/Datasets/koe/video/Frieren_S01E01.mkv\")\n",
    "\n",
    "temp_dir = input_file.parent / \"temp\"\n",
    "temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "path = Path(input_file)\n",
    "name = path.stem\n",
    "\n",
    "outputs_dir = path.parent / \"outputs\"\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_file = outputs_dir / (name + \"_condensed.wav\")\n",
    "output_file_all = outputs_dir / (name + \"_condensed_all.wav\")\n",
    "output_op = outputs_dir / (name + \"_op.wav\")\n",
    "output_ed = outputs_dir / (name + \"_ed.wav\")\n",
    "audio_file = temp_dir / (name + '.wav')\n",
    "\n",
    "if not audio_file.exists():\n",
    "    \n",
    "    ffprobe_output = ffprobe3.probe(str(path))    \n",
    "\n",
    "    audio_index = 0 #default to \n",
    "    for i in range(len(ffprobe_output.audio)):\n",
    "        s = ffprobe_output.audio[i]\n",
    "        tags = s.parsed_json['tags']\n",
    "        if \"language\" not in tags:\n",
    "            break\n",
    "        if tags[\"language\"] == \"jpn\":\n",
    "            audio_index = i\n",
    "            break\n",
    "\n",
    "    ffmpeg = (\n",
    "        FFmpeg()\n",
    "        .input(str(path))\n",
    "        .option(\"vn\")\n",
    "        .output(\n",
    "            temp_dir / (name + '.wav'),\n",
    "            map=[\"0:a:\" + str(audio_index)],\n",
    "            acodec=\"pcm_s16le\",\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        ffmpeg.execute()\n",
    "    except FFmpegError as exception:\n",
    "        print(\"- Message from ffmpeg:\", exception.message)\n",
    "        print(\"- Arguments to execute ffmpeg:\", \" \".join(exception.arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\") #test cpu inference\n",
    "#torch.set_num_threads(16)\n",
    "\n",
    "model_path = Path(\"H:/Documents/Dev/ML/Koe.moe/checkpoints/latest.pt\")\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "bs = 128 if device.type != \"cpu\" else 32\n",
    "sr = 16000\n",
    "len_sec = 6\n",
    "\n",
    "len_samples = len_sec*sr\n",
    "\n",
    "o,o_sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "y = librosa.resample(o, orig_sr=o_sr, target_sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pad end to get non-fractional number of clips\n",
    "num_clips = int(math.ceil(y.shape[0]/len_samples))\n",
    "missing_samples = num_clips*len_samples - y.shape[0]\n",
    "\n",
    "zeros = np.zeros(missing_samples)\n",
    "y = np.append(y, zeros, axis=0)\n",
    "\n",
    "#reshape into clip length\n",
    "_y = y.reshape((num_clips, len_samples))\n",
    "\n",
    "#generate inputs\n",
    "inputs = []\n",
    "for i in range(_y.shape[0]):\n",
    "    melspec = librosa.feature.melspectrogram(y=_y[i], sr=sr, hop_length=160)\n",
    "    melspec = librosa.power_to_db(melspec, ref=np.max)\n",
    "    inputs.append(melspec)\n",
    "\n",
    "inputs = np.array(inputs).astype(np.float32)\n",
    "inputs = torch.from_numpy(np.array([inputs])).to(device)\n",
    "inputs = inputs.permute(1, 0, 2, 3)\n",
    "\n",
    "batches = int(math.ceil(inputs.shape[0]/bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:02<00:00,  1.30it/s]\n"
     ]
    }
   ],
   "source": [
    "print(\"Starting inference...\")\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for b in tqdm(range(batches)):\n",
    "        start = b*bs\n",
    "        end = (b+1)*bs if b != (batches - 1) else inputs.shape[0]\n",
    "        batch = inputs[start:end]\n",
    "        #print(\"Batch: \" + str(b+1))\n",
    "        outputs += model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1522.6005398571956#0.020259369801109036\n",
      "1522.800666560745#0.02114367188575367\n",
      "1523.0003833848518#0.021789140300825238\n",
      "1523.2006167120767#0.022567254196231564\n",
      "1523.4006519800053#0.02349877979916831\n",
      "1523.6006028201896#0.024153695519392688\n",
      "1523.8007920145058#0.024778681326036653\n",
      "1524.0010564375668#0.024778681326036653\n",
      "1524.2009409119376#0.024778681326036653\n",
      "1524.4008011403494#0.024778681326036653\n",
      "1524.6008257330395#0.024778681326036653\n",
      "1524.8010675554165#0.024778681326036653\n",
      "1525.001074468717#0.024778681326036653\n",
      "1525.200709299976#0.024778681326036653\n",
      "1525.4010354021564#0.024778681326036653\n",
      "1525.6012285308911#0.024778681326036653\n",
      "1525.8009280282072#0.024778681326036653\n",
      "1526.001001326833#0.024778681326036653\n",
      "1526.2009019705467#0.024778681326036653\n",
      "1526.4008160014637#0.024778681326036653\n",
      "1526.6011032105423#0.024778681326036653\n",
      "1526.800942767784#0.024778681326036653\n",
      "1527.0016444414855#0.024778681326036653\n",
      "1527.2010650080629#0.024778681326036653\n",
      "1527.4008839109913#0.024778681326036653\n",
      "1527.600942652393#0.024778681326036653\n",
      "1527.800913237501#0.024778681326036653\n",
      "1528.0012816814706#0.024778681326036653\n",
      "1528.2010088984855#0.024778681326036653\n",
      "1528.4011345388367#0.024778681326036653\n",
      "1528.6011229605415#0.024778681326036653\n",
      "1528.8012380797416#0.024778681326036653\n",
      "1529.0007416788953#0.024778681326036653\n",
      "1529.2010736409575#0.024778681326036653\n",
      "1529.4012437939643#0.024778681326036653\n",
      "1529.6011598624289#0.024778681326036653\n",
      "1529.8016468659043#0.024778681326036653\n",
      "1530.0013482640497#0.024778681326036653\n",
      "1530.2009137730115#0.02376324306242168\n",
      "1530.4008966629394#0.022865566440547505\n",
      "1530.6007968104445#0.022186339910452563\n",
      "1530.801119675208#0.02105192340289553\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def samples_to_t(samples):\n",
    "    return samples/sr\n",
    "\n",
    "@dataclass\n",
    "class LabelData:\n",
    "    name: str\n",
    "    threshold: float\n",
    "    padding: float\n",
    "    moving_avg: bool = False\n",
    "    moving_avg_n: int = 2\n",
    "    relative_to: int = -1\n",
    "    verbose: bool = False\n",
    "    events: list[tuple] = field(default_factory=list)\n",
    "\n",
    "events = {\"Speech\": [], \"OPED\": []}\n",
    "\n",
    "classes = int(outputs[0].shape[1] / 3)\n",
    "\n",
    "samples_per_segment = len_samples/outputs[0].shape[0]\n",
    "\n",
    "class_map = { #.475 2747\n",
    "    0: LabelData(\"Speech\", 0.455, [.7, .9], moving_avg=False, verbose=False),\n",
    "    1: LabelData(\"OPED\", .02, [.3, .3], moving_avg=True, relative_to=0, verbose=True)\n",
    "}\n",
    "\n",
    "for c in range(classes):\n",
    "    label_class = class_map[c]\n",
    "    \n",
    "    moving_avg_terms = int(label_class.moving_avg_n*outputs[0].shape[0])\n",
    "    last_n = []\n",
    "    \n",
    "    total_correct = 0\n",
    "    for i in range(len(outputs)):\n",
    "        clip = outputs[i]\n",
    "        clip_sample_offset = i*clip.shape[0]*samples_per_segment\n",
    "        for time_step in range(clip.shape[0]):\n",
    "            step_start_samples = clip_sample_offset + samples_per_segment*time_step\n",
    "            valid = clip[time_step][0 + c*3].item()\n",
    "            \n",
    "            if label_class.relative_to > -1:\n",
    "                valid = max(0, (valid - clip[time_step][label_class.relative_to*3].item()))\n",
    "                \n",
    "            #moving average\n",
    "            if label_class.moving_avg:\n",
    "                if len(last_n) == moving_avg_terms:\n",
    "                    new_valid = (valid + sum(last_n))/moving_avg_terms\n",
    "                    for j in range(0, moving_avg_terms - 1):\n",
    "                        last_n[j] = last_n[j+1]\n",
    "                    last_n[-1] = valid\n",
    "                    valid = new_valid\n",
    "                else:\n",
    "                    last_n.append(valid)\n",
    "            \n",
    "            start = clip[time_step][1 + c*3].item()\n",
    "            stop = clip[time_step][2 + c*3].item()\n",
    "            \n",
    "            if valid >= label_class.threshold and start < stop:\n",
    "                start_time = step_start_samples + start*samples_per_segment\n",
    "                stop_time = step_start_samples + stop*samples_per_segment\n",
    "                label_class.events.append((valid, start_time, stop_time))\n",
    "                if label_class.verbose: print(f'{samples_to_t(start_time)}#{valid}')\n",
    "\n",
    "for idx, label_class in class_map.items():\n",
    "    for i in range(0, len(label_class.events)):\n",
    "        curr = label_class.events[i]\n",
    "        new_start = max(0, curr[1] - sr*label_class.padding[0]) \n",
    "        new_stop = min(y.shape[0] - 1, curr[1] + sr*label_class.padding[1])\n",
    "        label_class.events[i] = (curr[0], new_start, new_stop)\n",
    "        \n",
    "#Otherwise the subsequent clip concatentation is very slow\n",
    "smoothing = .1\n",
    "for idx, label_class in class_map.items():\n",
    "    smoothed_events = []\n",
    "    previous_pointer = 0\n",
    "    for i in range(1, len(label_class.events)):\n",
    "        prev = label_class.events[previous_pointer]\n",
    "        curr = label_class.events[i]\n",
    "        if curr[1] - prev[2] <= smoothing:\n",
    "            label_class.events[i] = (curr[0], prev[1], curr[2])\n",
    "            label_class.events[previous_pointer] = None\n",
    "        previous_pointer = i\n",
    "    label_class.events = list(filter(lambda x: x, label_class.events))\n",
    "\n",
    "\n",
    "sr_correction = o_sr/sr\n",
    "all_speech = np.array([])\n",
    "speech_class = class_map[0]\n",
    "for i in range(0, len(speech_class.events)):\n",
    "        clip_start = speech_class.events[i][1]*sr_correction\n",
    "        clip_stop = speech_class.events[i][2]*sr_correction\n",
    "        clip = o[int(clip_start):int(clip_stop)]\n",
    "        all_speech = np.concatenate((all_speech, clip))\n",
    "\n",
    "sf.write(output_file, all_speech, o_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = np.array([])\n",
    "ed = np.array([])\n",
    "oped_class = class_map[1]\n",
    "for i in range(0, len(oped_class.events)):\n",
    "        clip_start = oped_class.events[i][1]*sr_correction\n",
    "        clip_stop = oped_class.events[i][2]*sr_correction\n",
    "        clip = o[int(clip_start):int(clip_stop)]\n",
    "        \n",
    "        if clip_start >= (y.shape[0]/2):\n",
    "            ed = np.concatenate((ed, clip))\n",
    "        else:\n",
    "            op = np.concatenate((op, clip))\n",
    "\n",
    "all = np.array([])\n",
    "\n",
    "if op.shape[0] > 0:\n",
    "    all = np.concatenate((all, op))\n",
    "    sf.write(output_op, op, o_sr)\n",
    "    \n",
    "all = np.concatenate((all, all_speech))\n",
    "    \n",
    "if ed.shape[0] > 0:\n",
    "    all = np.concatenate((all, ed))\n",
    "    sf.write(output_ed, ed, o_sr)\n",
    "    \n",
    "sf.write(output_file_all, all, o_sr)\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastxtend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
