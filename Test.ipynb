{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import math\n",
    "import torch\n",
    "import ffprobe3\n",
    "import shutil\n",
    "import soundfile as sf\n",
    "import srt\n",
    "import bisect\n",
    "import re\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from ffmpeg import FFmpeg, FFmpegError # type: ignore\n",
    "from IPython.display import Audio\n",
    "from dataclasses import dataclass, field\n",
    "from functools import partial\n",
    "\n",
    "input_file = Path(\"X:/ML/Datasets/koe/video/Frieren_S01E01.mkv\")\n",
    "\n",
    "subs_file = Path(\"X:/ML/Datasets/koe/subs/Frieren_S01E01.srt\")\n",
    "\n",
    "temp_dir = input_file.parent / \"temp\"\n",
    "temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "path = Path(input_file)\n",
    "name = path.stem\n",
    "\n",
    "outputs_dir = path.parent / \"outputs\"\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_file = outputs_dir / (name + \"_condensed.wav\")\n",
    "output_file_all = outputs_dir / (name + \"_condensed_all.wav\")\n",
    "output_op = outputs_dir / (name + \"_op.wav\")\n",
    "output_ed = outputs_dir / (name + \"_ed.wav\")\n",
    "audio_file = temp_dir / (name + '.wav')\n",
    "\n",
    "if not audio_file.exists():\n",
    "    \n",
    "    ffprobe_output = ffprobe3.probe(str(path))    \n",
    "\n",
    "    audio_index = 0 #default to \n",
    "    for i in range(len(ffprobe_output.audio)):\n",
    "        s = ffprobe_output.audio[i]\n",
    "        tags = s.parsed_json['tags']\n",
    "        if \"language\" not in tags:\n",
    "            break\n",
    "        if tags[\"language\"] == \"jpn\":\n",
    "            audio_index = i\n",
    "            break\n",
    "\n",
    "    ffmpeg = (\n",
    "        FFmpeg()\n",
    "        .input(str(path))\n",
    "        .option(\"vn\")\n",
    "        .output(\n",
    "            temp_dir / (name + '.wav'),\n",
    "            map=[\"0:a:\" + str(audio_index)],\n",
    "            acodec=\"pcm_s16le\",\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        ffmpeg.execute()\n",
    "    except FFmpegError as exception:\n",
    "        print(\"- Message from ffmpeg:\", exception.message)\n",
    "        print(\"- Arguments to execute ffmpeg:\", \" \".join(exception.arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\") #test cpu inference\n",
    "#torch.set_num_threads(16)\n",
    "\n",
    "model_path = Path(\"H:/Documents/Dev/ML/Koe.moe/checkpoints/latest.pt\")\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "bs = 128 if device.type != \"cpu\" else 32\n",
    "sr = 16000\n",
    "len_sec = 6\n",
    "\n",
    "len_samples = len_sec*sr\n",
    "\n",
    "o,o_sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "y = librosa.resample(o, orig_sr=o_sr, target_sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pad end to get non-fractional number of clips\n",
    "num_clips = int(math.ceil(y.shape[0]/len_samples))\n",
    "missing_samples = num_clips*len_samples - y.shape[0]\n",
    "\n",
    "zeros = np.zeros(missing_samples)\n",
    "y = np.append(y, zeros, axis=0)\n",
    "\n",
    "#reshape into clip length\n",
    "_y = y.reshape((num_clips, len_samples))\n",
    "\n",
    "#generate inputs\n",
    "inputs = []\n",
    "for i in range(_y.shape[0]):\n",
    "    melspec = librosa.feature.melspectrogram(y=_y[i], sr=sr, hop_length=160)\n",
    "    melspec = librosa.power_to_db(melspec, ref=np.max)\n",
    "    inputs.append(melspec)\n",
    "\n",
    "inputs = np.array(inputs).astype(np.float32)\n",
    "inputs = torch.from_numpy(np.array([inputs])).to(device)\n",
    "inputs = inputs.permute(1, 0, 2, 3)\n",
    "\n",
    "batches = int(math.ceil(inputs.shape[0]/bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting inference...\")\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for b in tqdm(range(batches)):\n",
    "        start = b*bs\n",
    "        end = (b+1)*bs if b != (batches - 1) else inputs.shape[0]\n",
    "        batch = inputs[start:end]\n",
    "        #print(\"Batch: \" + str(b+1))\n",
    "        outputs += model(batch)\n",
    "    \n",
    "#outputs onto cpu\n",
    "for i in range(len(outputs)):\n",
    "    outputs[i] = outputs[i].cpu()\n",
    "\n",
    "outputs = np.array(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_sub_file(subs_file):\n",
    "    with open(subs_file) as f:\n",
    "        lines = f.readlines()\n",
    "    subs = list(srt.parse(\"\\n\".join(lines)))\n",
    "    starts = []\n",
    "    stops = [] \n",
    "    for s in subs:\n",
    "        content = s.content\n",
    "        table = content.maketrans(\"（）}{\", \"())(\") #Swap out alternative brackets for normal ones\n",
    "        content = content.translate(table)\n",
    "        content = re.sub(\"\\(.*?\\)\",\"\", content) #Get rid of all bracketed stuff\n",
    "        content = content.replace(\"♪\", \"\").replace(\"～\", \"\").replace(\"…\", \"\").strip()\n",
    "        if content:\n",
    "            starts.append(s.start.total_seconds())\n",
    "            stops.append(s.end.total_seconds())\n",
    "    return starts, stops\n",
    "\n",
    "sub_starts, sub_stops = parse_sub_file(subs_file)\n",
    "\n",
    "def print_sub_times():\n",
    "    time_step = 6.0/outputs[0].shape[0]\n",
    "    idx = 0\n",
    "    time = 0\n",
    "    while(idx < len(sub_starts)):\n",
    "        if time >= sub_starts[idx] and time <= sub_stops[idx]:\n",
    "            print(f'1.5')\n",
    "            time += time_step\n",
    "        elif time < sub_starts[idx]:\n",
    "            print(f'0')\n",
    "            time += time_step\n",
    "        else: #must be greater than sub_stops[idx]\n",
    "            idx += 1 \n",
    "\n",
    "#print_sub_times()\n",
    "\n",
    "def map_to_range(value, in_min, in_max, out_max=1.0, out_min=0):\n",
    "    return out_min + ((value - in_min)/(in_max - in_min))*(out_max - out_min)\n",
    "\n",
    "def samples_to_t(samples):\n",
    "    return samples/sr\n",
    "\n",
    "@dataclass\n",
    "class LabelData:\n",
    "    name: str\n",
    "    threshold: float\n",
    "    padding: float\n",
    "    use_momentum: bool = False\n",
    "    momentum_terms: int = 30 #in terms of time steps\n",
    "    mu: float = .3\n",
    "    relative_to: int = -1\n",
    "    verbose: bool = False\n",
    "    events: list[tuple] = field(default_factory=list)\n",
    "\n",
    "events = {\"Speech\": [], \"OPED\": []}\n",
    "\n",
    "classes = int(outputs[0].shape[1] / 3)\n",
    "\n",
    "samples_per_segment = len_samples/outputs[0].shape[0]\n",
    "\n",
    "class_map = { #0.69\n",
    "    0: LabelData(\"Speech\", .76, [.5, .5], relative_to=1, use_momentum=False, verbose=True),\n",
    "    1: LabelData(\"OPED\", .0001, [.3, .3], relative_to=0, use_momentum=False, verbose=False)\n",
    "}\n",
    "\n",
    "for c in range(classes):\n",
    "    min_val = np.min(outputs[:, :, c*3])\n",
    "    max_val = np.max(outputs[:, :, c*3])\n",
    "    \n",
    "    outputs[:, :, c*3] = (outputs[:, :, c*3] - min_val)/(max_val - min_val)\n",
    "    \n",
    "    label_class = class_map[c]\n",
    "    thresh = label_class.threshold\n",
    "    momentum_terms = label_class.momentum_terms\n",
    "    last_n = []\n",
    "    \n",
    "    total_correct = 0\n",
    "    for i in range(len(outputs)):\n",
    "        clip = outputs[i]\n",
    "        clip_sample_offset = i*clip.shape[0]*samples_per_segment\n",
    "        for time_step in range(clip.shape[0]):\n",
    "            step_start_samples = clip_sample_offset + samples_per_segment*time_step\n",
    "            valid = clip[time_step][0 + c*3]\n",
    "            \n",
    "            if label_class.relative_to > -1:\n",
    "                valid = max(0, (valid - clip[time_step][label_class.relative_to*3]))\n",
    "                \n",
    "            #momentum, (kinda...)\n",
    "            if label_class.use_momentum:\n",
    "                \n",
    "                if len(last_n) == momentum_terms:\n",
    "                    kinetic_erg = 0\n",
    "                    mu = label_class.mu\n",
    "                    m = 1000 #1kg\n",
    "                    dist_mult = 1/outputs[0].shape[0]\n",
    "                    \n",
    "                    for j in range(0, momentum_terms):\n",
    "                        #get original kinetic eng of term, subtract energy lost due to friction, assume perfect energy transfer to next label\n",
    "                        dist = (momentum_terms - j)*dist_mult\n",
    "                        v_i = last_n[j]*1.5 #Treat the probability as a velocity\n",
    "                        #m = map_to_range(last_n[j], thresh, 1.0)*mass_mult if last_n[j] >= thresh else 0\n",
    "                        ke = .5*m*((v_i)**2) #we can play with velocity term\n",
    "                        n_f = m*9.81 #f_norm = m*g\n",
    "                        f_fr = mu*n_f #F_fr = mu*f_norm\n",
    "                        w_fr = -f_fr*dist #work = F*d AKA the net_work done on the system\n",
    "                        #now solve backwards to find KE after friction (work energy theorem)\n",
    "                        ke_f = max(0, w_fr + ke) #clamp to zero since we assume that it moves the full distance when it could stop before\n",
    "                        kinetic_erg += ke_f\n",
    "                \n",
    "                    v_i = valid\n",
    "                    ke = .5*m*((v_i)**2)\n",
    "                    new_ke = ke + kinetic_erg\n",
    "                    new_valid = (new_ke/(.5*m))**(.5) #solve for new \"velocity\"\n",
    "                    new_valid = min(1.0, new_valid) #clamp to 1\n",
    "                    \n",
    "                    for j in range(0, momentum_terms - 1):\n",
    "                        last_n[j] = last_n[j+1]\n",
    "                    \n",
    "                    last_n[-1] = (valid + new_valid)/2\n",
    "                    valid = new_valid\n",
    "                else:\n",
    "                    last_n.append(valid)\n",
    "            \n",
    "            start = clip[time_step][1 + c*3]\n",
    "            stop = clip[time_step][2 + c*3]\n",
    "            \n",
    "            if valid >= thresh and start < stop:\n",
    "                start_time = step_start_samples + start*samples_per_segment\n",
    "                stop_time = step_start_samples + stop*samples_per_segment\n",
    "                label_class.events.append((valid, start_time, stop_time))\n",
    "            if label_class.verbose: print(f'{valid}')\n",
    "\n",
    "for idx, label_class in class_map.items():\n",
    "    for i in range(0, len(label_class.events)):\n",
    "        curr = label_class.events[i]\n",
    "        new_start = max(0, curr[1] - sr*label_class.padding[0]) \n",
    "        new_stop = min(y.shape[0] - 1, curr[1] + sr*label_class.padding[1])\n",
    "        label_class.events[i] = (curr[0], new_start, new_stop)\n",
    "        \n",
    "#Otherwise the subsequent clip concatentation is very slow\n",
    "smoothing = .1\n",
    "for idx, label_class in class_map.items():\n",
    "    smoothed_events = []\n",
    "    previous_pointer = 0\n",
    "    for i in range(1, len(label_class.events)):\n",
    "        prev = label_class.events[previous_pointer]\n",
    "        curr = label_class.events[i]\n",
    "        if curr[1] - prev[2] <= smoothing:\n",
    "            label_class.events[i] = (curr[0], prev[1], curr[2])\n",
    "            label_class.events[previous_pointer] = None\n",
    "        previous_pointer = i\n",
    "    label_class.events = list(filter(lambda x: x, label_class.events))\n",
    "\n",
    "\n",
    "sr_correction = o_sr/sr\n",
    "all_speech = np.array([])\n",
    "speech_class = class_map[0]\n",
    "for i in range(0, len(speech_class.events)):\n",
    "        clip_start = speech_class.events[i][1]*sr_correction\n",
    "        clip_stop = speech_class.events[i][2]*sr_correction\n",
    "        clip = o[int(clip_start):int(clip_stop)]\n",
    "        all_speech = np.concatenate((all_speech, clip))\n",
    "\n",
    "sf.write(output_file, all_speech, o_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = np.array([])\n",
    "ed = np.array([])\n",
    "oped_class = class_map[1]\n",
    "for i in range(0, len(oped_class.events)):\n",
    "        clip_start = oped_class.events[i][1]*sr_correction\n",
    "        clip_stop = oped_class.events[i][2]*sr_correction\n",
    "        clip = o[int(clip_start):int(clip_stop)]\n",
    "        \n",
    "        if clip_start >= (y.shape[0]/2):\n",
    "            ed = np.concatenate((ed, clip))\n",
    "        else:\n",
    "            op = np.concatenate((op, clip))\n",
    "\n",
    "all = np.array([])\n",
    "\n",
    "if op.shape[0] > 0:\n",
    "    all = np.concatenate((all, op))\n",
    "    sf.write(output_op, op, o_sr)\n",
    "    \n",
    "all = np.concatenate((all, all_speech))\n",
    "    \n",
    "if ed.shape[0] > 0:\n",
    "    all = np.concatenate((all, ed))\n",
    "    sf.write(output_ed, ed, o_sr)\n",
    "    \n",
    "sf.write(output_file_all, all, o_sr)\n",
    "shutil.rmtree(temp_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastxtend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
