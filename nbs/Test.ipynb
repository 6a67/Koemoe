{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import math\n",
    "import torch\n",
    "import ffprobe3\n",
    "import shutil\n",
    "import soundfile as sf\n",
    "\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "from ffmpeg import FFmpeg, FFmpegError # type: ignore\n",
    "from dataclasses import dataclass, field\n",
    "\n",
    "input_file = Path(\"X:/ML/Datasets/koe/video/Frieren_S01E01.mkv\")\n",
    "\n",
    "temp_dir = input_file.parent / \"temp\"\n",
    "temp_dir.mkdir(exist_ok=True)\n",
    "\n",
    "path = Path(input_file)\n",
    "name = path.stem\n",
    "\n",
    "outputs_dir = path.parent / \"outputs\"\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "output_file = outputs_dir / (name + \"_condensed.wav\")\n",
    "output_file_all = outputs_dir / (name + \"_condensed_all.wav\")\n",
    "output_op = outputs_dir / (name + \"_op.wav\")\n",
    "output_ed = outputs_dir / (name + \"_ed.wav\")\n",
    "audio_file = temp_dir / (name + '.wav')\n",
    "\n",
    "if not audio_file.exists():\n",
    "    ffprobe_output = ffprobe3.probe(str(path))    \n",
    "\n",
    "    audio_index = 0 #default to \n",
    "    for i in range(len(ffprobe_output.audio)):\n",
    "        s = ffprobe_output.audio[i]\n",
    "        tags = s.parsed_json['tags']\n",
    "        if \"language\" not in tags:\n",
    "            break\n",
    "        if tags[\"language\"] == \"jpn\":\n",
    "            audio_index = i\n",
    "            break\n",
    "\n",
    "    ffmpeg = (\n",
    "        FFmpeg()\n",
    "        .input(str(path))\n",
    "        .option(\"vn\")\n",
    "        .output(\n",
    "            temp_dir / (name + '.wav'),\n",
    "            map=[\"0:a:\" + str(audio_index)],\n",
    "            acodec=\"pcm_s16le\",\n",
    "        )\n",
    "    )\n",
    "    try:\n",
    "        ffmpeg.execute()\n",
    "    except FFmpegError as exception:\n",
    "        print(\"- Message from ffmpeg:\", exception.message)\n",
    "        print(\"- Arguments to execute ffmpeg:\", \" \".join(exception.arguments))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\") #test cpu inference\n",
    "#torch.set_num_threads(16)\n",
    "\n",
    "model_path = Path(\"H:/Documents/Dev/ML/Koe.moe/checkpoints/latest.pt\")\n",
    "\n",
    "model = torch.load(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "bs = 128 if device.type != \"cpu\" else 32\n",
    "sr = 16000\n",
    "len_sec = 6\n",
    "\n",
    "len_samples = len_sec*sr\n",
    "\n",
    "o,o_sr = librosa.load(audio_file, sr=None)\n",
    "\n",
    "y = librosa.resample(o, orig_sr=o_sr, target_sr=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#pad end to get non-fractional number of clips\n",
    "num_clips = int(math.ceil(y.shape[0]/len_samples))\n",
    "missing_samples = num_clips*len_samples - y.shape[0]\n",
    "\n",
    "zeros = np.zeros(missing_samples)\n",
    "y = np.append(y, zeros, axis=0)\n",
    "\n",
    "#reshape into clip length\n",
    "_y = y.reshape((num_clips, len_samples))\n",
    "\n",
    "#generate inputs\n",
    "inputs = []\n",
    "for i in range(_y.shape[0]):\n",
    "    melspec = librosa.feature.melspectrogram(y=_y[i], sr=sr, hop_length=160)\n",
    "    melspec = librosa.power_to_db(melspec, ref=np.max)\n",
    "    inputs.append(melspec)\n",
    "\n",
    "inputs = np.array(inputs).astype(np.float32)\n",
    "inputs = torch.from_numpy(np.array([inputs])).to(device)\n",
    "inputs = inputs.permute(1, 0, 2, 3)\n",
    "\n",
    "batches = int(math.ceil(inputs.shape[0]/bs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9/9 [00:10<00:00,  1.20s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Starting inference...\")\n",
    "outputs = []\n",
    "with torch.no_grad():\n",
    "    for b in tqdm(range(batches)):\n",
    "        start = b*bs\n",
    "        end = (b+1)*bs if b != (batches - 1) else inputs.shape[0]\n",
    "        batch = inputs[start:end]\n",
    "        #print(\"Batch: \" + str(b+1))\n",
    "        outputs += model(batch)\n",
    "    \n",
    "#outputs onto cpu\n",
    "for i in range(len(outputs)):\n",
    "    outputs[i] = outputs[i].cpu()\n",
    "\n",
    "outputs = np.array(outputs)\n",
    "steps = outputs.reshape((outputs.shape[0]*outputs.shape[1], outputs.shape[2])) #flatten since we do not need the extra clip dimension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subs_file = Path(\"X:/ML/Datasets/koe/subs/Frieren_S01E01.srt\")\n",
    "#\n",
    "# def parse_sub_file(subs_file):\n",
    "#     with open(subs_file) as f:\n",
    "#         lines = f.readlines()\n",
    "#     subs = list(srt.parse(\"\\n\".join(lines)))\n",
    "#     starts = []\n",
    "#     stops = [] \n",
    "#     for s in subs:\n",
    "#         content = s.content\n",
    "#         table = content.maketrans(\"（）}{\", \"())(\") #Swap out alternative brackets for normal ones\n",
    "#         content = content.translate(table)\n",
    "#         content = re.sub(\"\\(.*?\\)\",\"\", content) #Get rid of all bracketed stuff\n",
    "#         content = content.replace(\"♪\", \"\").replace(\"～\", \"\").replace(\"…\", \"\").strip()\n",
    "#         if content:\n",
    "#             starts.append(s.start.total_seconds())\n",
    "#             stops.append(s.end.total_seconds())\n",
    "#     return starts, stops\n",
    "\n",
    "# sub_starts, sub_stops = parse_sub_file(subs_file)\n",
    "\n",
    "# def print_sub_times():\n",
    "#     time_step = 6.0/outputs[0].shape[0]\n",
    "#     idx = 0\n",
    "#     time = 0\n",
    "#     while(idx < len(sub_starts)):\n",
    "#         if time >= sub_starts[idx] and time <= sub_stops[idx]:\n",
    "#             print(f'1.5')\n",
    "#             time += time_step\n",
    "#         elif time < sub_starts[idx]:\n",
    "#             print(f'0')\n",
    "#             time += time_step\n",
    "#         else: #must be greater than sub_stops[idx]\n",
    "#             idx += 1 \n",
    "\n",
    "# #print_sub_times()\n",
    "\n",
    "def map_to_range(value, in_min, in_max, out_max=1.0, out_min=0):\n",
    "    return out_min + ((value - in_min)/(in_max - in_min))*(out_max - out_min)\n",
    "\n",
    "def samples_to_t(samples):\n",
    "    return samples/sr\n",
    "\n",
    "@dataclass\n",
    "class LabelData:\n",
    "    name: str\n",
    "    threshold: float\n",
    "    padding: float\n",
    "    smooth: bool = False\n",
    "    smooth_n: int = 30 #in terms of time steps\n",
    "    smooth_und_weight: float = 1.0\n",
    "    smooth_over_weight: float = 1.0\n",
    "    relative_to: int = -1\n",
    "    verbose: bool = False\n",
    "    events: list[tuple] = field(default_factory=list)\n",
    "\n",
    "events = {\"Speech\": [], \"OPED\": []}\n",
    "\n",
    "classes = int(outputs[0].shape[1] / 3)\n",
    "\n",
    "samples_per_segment = len_samples/outputs[0].shape[0]\n",
    "\n",
    "class_map = { #0.69\n",
    "    0: LabelData(\"Speech\", .51, [.75, .75], relative_to=1,verbose=False),\n",
    "    1: LabelData(\"OPED\", .25, [.3, .3], relative_to=0, smooth=True, smooth_n=60, verbose=False)\n",
    "}\n",
    "\n",
    "rng = np.random.default_rng()\n",
    "\n",
    "for c in range(classes):\n",
    "    min_val = np.min(steps[:, c*3])\n",
    "    max_val = np.max(steps[:, c*3])\n",
    "    \n",
    "    steps[:, c*3] = (steps[:, c*3] - min_val)/(max_val - min_val)\n",
    "    \n",
    "    label_class = class_map[c]\n",
    "    thresh = label_class.threshold\n",
    "    smooth_terms = label_class.smooth_n\n",
    "    \n",
    "    \n",
    "    for i in range(len(steps)):\n",
    "        step = steps[i]\n",
    "        step_samples_offset = i*samples_per_segment\n",
    "        valid = step[c*3]\n",
    "        \n",
    "        if label_class.relative_to > -1:\n",
    "            valid = max(0, (valid - step[label_class.relative_to*3]))\n",
    "            \n",
    "        #Smoothing, constantly changing/playing with\n",
    "        if label_class.smooth:\n",
    "            last_n = []\n",
    "            front = [] #n/2 terms before time_step\n",
    "            back = [] #n/2 terms after time_step\n",
    "            \n",
    "            term_length = smooth_terms/2\n",
    "            while (i - term_length < 0 or i + term_length >= len(steps)):\n",
    "                term_length -= 1\n",
    "                \n",
    "            start = int(i - term_length)\n",
    "            end = int(i + term_length)\n",
    "            \n",
    "            if start < i:\n",
    "                x = steps[start:i, c*3]\n",
    "                front += x.reshape((i - start)).tolist()\n",
    "            if end > (i+1):\n",
    "                x = steps[(i+1):end, c*3]\n",
    "                back += x.reshape((end - (i + 1))).tolist()\n",
    "                \n",
    "            last_n += front\n",
    "            last_n += [valid]\n",
    "            last_n += back\n",
    "            if len(last_n) > 1:\n",
    "                vals = np.array(last_n)\n",
    "                \n",
    "                under = vals < thresh\n",
    "                over = vals >= thresh\n",
    "                \n",
    "                vals[under] = vals[under]*label_class.smooth_und_weight\n",
    "                vals[over] = vals[over]*label_class.smooth_over_weight\n",
    "                x = len(last_n)/2 - np.abs(np.linspace(-1*int(len(last_n)/2), int(len(last_n)/2), vals.shape[0]))\n",
    "                x = x**.5\n",
    "                \n",
    "                if x.max() == x.min():\n",
    "                    x = np.ones(x.shape)\n",
    "                    _x = np.ones(x.shape)\n",
    "                else:\n",
    "                    x = (x - x.min()) / (x.max() - x.min())\n",
    "                    _x = x.max() - x\n",
    "                \n",
    "                val_over = (vals[over]*x[over]).sum()\n",
    "                val_under = (vals[under]*_x[under]).sum()\n",
    "                new_valid = max(0, val_over + val_under)\n",
    "                valid = min(1, new_valid)\n",
    "        \n",
    "        start = step[1 + c*3]\n",
    "        stop = step[2 + c*3]\n",
    "        \n",
    "        if valid >= thresh and start < stop:\n",
    "            start_time = step_samples_offset + start*samples_per_segment\n",
    "            stop_time = step_samples_offset + stop*samples_per_segment\n",
    "            label_class.events.append((valid, start_time, stop_time))\n",
    "        if label_class.verbose: print(f'{valid}')\n",
    "\n",
    "for idx, label_class in class_map.items():\n",
    "    for i in range(0, len(label_class.events)):\n",
    "        curr = label_class.events[i]\n",
    "        new_start = max(0, curr[1] - sr*label_class.padding[0]) \n",
    "        new_stop = min(y.shape[0] - 1, curr[1] + sr*label_class.padding[1])\n",
    "        label_class.events[i] = (curr[0], new_start, new_stop)\n",
    "        \n",
    "#Otherwise the subsequent clip concatentation is very slow\n",
    "smoothing = .1\n",
    "for idx, label_class in class_map.items():\n",
    "    smoothed_events = []\n",
    "    previous_pointer = 0\n",
    "    for i in range(1, len(label_class.events)):\n",
    "        prev = label_class.events[previous_pointer]\n",
    "        curr = label_class.events[i]\n",
    "        if curr[1] - prev[2] <= smoothing:\n",
    "            label_class.events[i] = (curr[0], prev[1], curr[2])\n",
    "            label_class.events[previous_pointer] = None\n",
    "        previous_pointer = i\n",
    "    label_class.events = list(filter(lambda x: x, label_class.events))\n",
    "\n",
    "\n",
    "\n",
    "sampled_idx = np.array([]) #build a list of samples that we included, so we can correctly build the oped included version without duplication\n",
    "sr_correction = o_sr/sr\n",
    "all_speech = np.array([])\n",
    "speech_class = class_map[0]\n",
    "for i in range(0, len(speech_class.events)): \n",
    "    clip_start = speech_class.events[i][1]*sr_correction\n",
    "    clip_stop = speech_class.events[i][2]*sr_correction\n",
    "    clip = o[int(clip_start):(int(clip_stop)+1)]\n",
    "    \n",
    "    all_samples = np.linspace(int(clip_start), int(clip_stop), num=(int(clip_stop) - int(clip_start) + 1), dtype=np.uint32)\n",
    "    sampled_idx = np.concatenate((sampled_idx, all_samples))\n",
    "    \n",
    "    all_speech = np.concatenate((all_speech, clip))\n",
    "\n",
    "sf.write(output_file, all_speech, o_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [],
   "source": [
    "op = np.array([])\n",
    "ed = np.array([])\n",
    "oped_class = class_map[1]\n",
    "for i in range(0, len(oped_class.events)):\n",
    "    clip_start = oped_class.events[i][1]*sr_correction\n",
    "    clip_stop = oped_class.events[i][2]*sr_correction\n",
    "    clip = o[int(clip_start):int(clip_stop)+1]\n",
    "    \n",
    "    all_samples = np.linspace(int(clip_start), int(clip_stop), num=(int(clip_stop) - int(clip_start) + 1), dtype=np.uint32)\n",
    "    sampled_idx = np.concatenate((sampled_idx, all_samples))\n",
    "    \n",
    "    if clip_start >= (y.shape[0]/2):\n",
    "        ed = np.concatenate((ed, clip))\n",
    "    else:\n",
    "        op = np.concatenate((op, clip))\n",
    "\n",
    "\n",
    "if op.shape[0] > 0:\n",
    "    sf.write(output_op, op, o_sr)\n",
    "    \n",
    "if ed.shape[0] > 0:\n",
    "    sf.write(output_ed, ed, o_sr)\n",
    "\n",
    "\n",
    "all = np.array([])\n",
    "\n",
    "def consecutive(data, stepsize=1):\n",
    "    return np.split(data, np.where(np.diff(data) != stepsize)[0]+1)\n",
    "\n",
    "sampled_idx = np.unique(sampled_idx)\n",
    "np.sort(sampled_idx)\n",
    "\n",
    "sampled_idx = consecutive(sampled_idx)\n",
    "\n",
    "for i in range(len(sampled_idx)):\n",
    "    segment = sampled_idx[i]\n",
    "    start = int(segment[0])\n",
    "    stop = int(segment[-1])\n",
    "    all = np.concatenate((all, o[start:stop + 1]))\n",
    "\n",
    "sf.write(output_file_all, all, o_sr)\n",
    "shutil.rmtree(temp_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fastxtend",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
